{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wrangling Report\n",
    "**Gbenga Thompson Awojinrin**\n",
    "\n",
    "In carrying out this project, I had to carry out wrangling steps to obtain 2 datasets, the twitter_archive_master.csv and image_archive_master.csv datasets. All the wrangling procedures observed in order to accomplish this were carried out using the Python programming language in a Jupyter notebook and are outlined below: \n",
    "\n",
    "---\n",
    "### Gathering the data\n",
    "- Downloading the twitter_archive_enhanced dataset manually\n",
    "- Downloading the image_predictions dataset programmatically\n",
    "- Putting checks in place to ensure that if the notebook is ever run in an environment without these 2 files, it downloads them programmatically.\n",
    "- I then proceeded to setup my Twitter API object with the tweepy library, authenticating it with access keys gotten from the [Twitter developer portal](https://developer.twitter.com/en/)\n",
    "- Using the .lookup_statuses method available to the API object, I query the Twitter API in batches, allowing me to get the 2000+ tweets without getting stopped by the rate limit. The query responses are saved as json files, allowing me to perpetually access them without having to query the Twitter API everytime I run the notebook.\n",
    "- tweet_id, retweet_count and favorite_count are extracted from these json files, saved as a tweet_json.txt file, and then reloaded into the notebook as tweet_json, a pandas DataFrame\n",
    "\n",
    "---\n",
    "### Data Assessment and Cleaning\n",
    "The data was then inspected for tidiness and quality issues that would have made analysis difficult. Listed below are the issues that were discovered.  \n",
    "Using the Define-Code-Test framework, these issues were resolved respectively as tabulated below:\n",
    "\n",
    "|  | **Data Assessment Step** | **Data Cleaning Step** |\n",
    "|---|:---|:---|\n",
    "| **Tidiness** | doggo, floofer, pupper and puppo columns of the archive_enhanced table all represent different values of a single variable, the dog stage | Melt the rows into one, making sure rows where all values are none is represented as None |\n",
    "| **Tidiness** | The tweet_json variables should be part of the archive_enhanced table | Merge the tweet_json_clean dataframe with archive_enhanced_clean using pandas merge function |\n",
    "| **Quality** | NA values in retweet_count and fav_count columns of the table after it is merged with the tweet_json table | Drop rows with NA values in these columns |\n",
    "| **Quality** | The values in the source column are not properly formatted because they are still surrounded by html tags | Use string slicing to retrieve the text between the tags |\n",
    "| **Quality** | expanded_url column contains links to Twitter and non-Twitter pages, e.g https://vine.co/v/iiLjKuYJpr | Replace the non-Twitter urls with the Twitter version by joining the https://twitter.com/dog_rates/status/ to the tweet_id |\n",
    "| **Quality** | Some of the tweets in the archive_enhanced dataframe are not original tweets, but retweets of other tweets | Drop records that have a non-NAN value for retweeted_status_id |\n",
    "| **Quality** | After resolving #3, retweeted_status_id, retweeted_user_id and retweeted_status_timestamp columns now contain nan values only | Drop the affected columns using the pandas drop function |\n",
    "| **Quality** | Erroneous datatype for timestamp column | Convert the timestamp column to Timestamp object using the pandas.to_datetime function |\n",
    "| **Quality** | Incorrect rating_numerator and rating_denominator values extracted from text in some records, e.g, value of 0 in rating_denominator column | Use new regex patterns to extract rating_numerator and rating_denominator values from the text column |\n",
    "| **Quality** | Prediction values are completely lowercase for some while others are titlecase | Convert everything to lowercase for uniformity and consistency |\n",
    "| **Quality** | Inconsistent records between the archive_enhanced and image_predictions tables | Drop records with tweet_ids that are not common in both tables |\n",
    "\n",
    "---\n",
    "### Storing the data\n",
    "To ensure that work done up to this point was not lost, I saved the gathered, assessed and cleaned datasets to csv files named twitter_archive_master and image_archive_master"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
